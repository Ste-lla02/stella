{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Colab/Jupyter: installa dipendenze\n",
        "#!pip install pytorch-grad-cam opencv-python pillow tqdm\n",
        "!pip install -qq grad-cam==1.4.6 torchinfo==1.7.1 #torch==1.12.1 torchvision==0.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHvfTebvLVOM",
        "outputId": "69b9e1ba-ffe1-41ee-ba31-20351ddb14bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt6WHpcuKkBa",
        "outputId": "2ef85920-5702-4226-e0cc-49e18c613ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  2.62it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# === 1) CONFIGURAZIONE ===\n",
        "INP_DIR = \"/content/input\"\n",
        "OUT_DIR    = \"/content/output\"\n",
        "MODEL_PATH = \"/content/predictors.pth\"\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "IMG_SIZE   = 224  # <-- ADATTA alle tue dimensioni di training\n",
        "#MEAN       = [0.485, 0.456, 0.406]   # <-- ADATTA se diverso in training\n",
        "#STD        = [0.229, 0.224, 0.225]\n",
        "\n",
        "MEAN = [0.5]   # <-- usa i valori del training se li conosci\n",
        "STD  = [0.5]\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# === 2) CARICA IL MODELLO ===\n",
        "# ESEMPIO: se avevi salvato solo state_dict su una ResNet18\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "model = resnet18(num_classes=2)  # <-- ADATTA il numero classi\n",
        "# 1) porta la prima conv a 1 canale\n",
        "model.conv1 = nn.Conv2d(\n",
        "    in_channels=1, out_channels=64,\n",
        "    kernel_size=7, stride=2, padding=3, bias=False\n",
        ")\n",
        "\n",
        "state = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
        "# Se salvato con keys 'state_dict' o simili, estrai la parte giusta\n",
        "if isinstance(state, dict) and \"state_dict\" in state:\n",
        "    state = {k.replace(\"model.\", \"\").replace(\"module.\", \"\"): v for k, v in state[\"state_dict\"].items()}\n",
        "elif isinstance(state, dict):\n",
        "    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
        "\n",
        "\n",
        "model.load_state_dict(state, strict=False)\n",
        "model.to(DEVICE).eval()\n",
        "\n",
        "# === 3) SCEGLI IL LAYER TARGET PER GRAD-CAM ===\n",
        "# ResNet: ultimo blocco conv\n",
        "target_layer = model.layer4[-1]\n",
        "\n",
        "# === 4) PREPROCESS ===\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),                     # da immagine L -> tensor [1,H,W]\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# === 5) GRAD-CAM ===\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "def load_image_RGB(path):\n",
        "    img = Image.open(path).convert(\"RGB\")   # se grayscale: .convert(\"L\") e poi .convert(\"RGB\") oppure gestisci 1 canale\n",
        "    img_np = np.array(img)\n",
        "    return img, img_np\n",
        "\n",
        "def load_image(path):\n",
        "    # input per il modello: 1 canale\n",
        "    img = Image.open(path).convert(\"L\")         # grayscale\n",
        "    gray_np = np.array(img)                     # [H,W], uint8\n",
        "\n",
        "    # per l’overlay (serve 3 canali in [0,1] per show_cam_on_image)\n",
        "    rgb_for_overlay = np.stack([gray_np, gray_np, gray_np], axis=-1)\n",
        "    return img, rgb_for_overlay\n",
        "\n",
        "def to_tensor(img_pil):\n",
        "    return preprocess(img_pil).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "def run_cam_on_image(img_pil, rgb_np, target_category=None):\n",
        "    input_tensor = to_tensor(img_pil)\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        with GradCAM(model=model, target_layers=[target_layer], use_cuda=(DEVICE==\"cuda\")) as cam:\n",
        "            targets = None\n",
        "            if target_category is not None:\n",
        "                targets = [ClassifierOutputTarget(int(target_category))]\n",
        "            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]  # [Hc, Wc] = [IMG_SIZE, IMG_SIZE]\n",
        "\n",
        "    # >>> resize CAM to original image size <<<\n",
        "    H, W = rgb_np.shape[:2]\n",
        "    cam_resized = cv2.resize(grayscale_cam, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    rgb_float = rgb_np.astype(np.float32) / 255.0\n",
        "    visualization = show_cam_on_image(rgb_float, cam_resized, use_rgb=True)\n",
        "    return cam_resized, visualization\n",
        "\n",
        "# === 6) LOOP SULLE IMMAGINI ===\n",
        "with torch.no_grad():\n",
        "  for fname in tqdm(sorted(os.listdir(INP_DIR))):\n",
        "      if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\", \".webp\")):\n",
        "          continue\n",
        "\n",
        "      fpath = os.path.join(INP_DIR, fname)\n",
        "      img_pil, rgb_np = load_image(fpath)\n",
        "\n",
        "      # 1) Predizione (senza grad)\n",
        "      with torch.no_grad():\n",
        "          tensor = to_tensor(img_pil)\n",
        "          logits = model(tensor)\n",
        "          pred_idx = int(torch.argmax(logits, dim=1).item())\n",
        "\n",
        "      # 2) Grad-CAM (con grad abilitato!)\n",
        "      #   - in caso qualche cella abbia disabilitato globalmente i grad,\n",
        "      #     forziamo l'abilitazione con enable_grad()\n",
        "      with torch.enable_grad():\n",
        "          grayscale_cam, overlay = run_cam_on_image(img_pil, rgb_np, target_category=pred_idx)\n",
        "\n",
        "      # 3) Salvataggi\n",
        "      base, ext = os.path.splitext(fname)\n",
        "      heatmap_path = os.path.join(OUT_DIR, f\"{base}_cam.png\")\n",
        "      overlay_path = os.path.join(OUT_DIR, f\"{base}_overlay.png\")\n",
        "\n",
        "      cv2.imwrite(heatmap_path, (grayscale_cam * 255).astype(np.uint8))\n",
        "      cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n"
      ]
    }
  ]
}